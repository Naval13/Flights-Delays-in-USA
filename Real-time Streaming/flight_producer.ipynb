{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIT5202 - Data Processing for Big Data S2 2021 \n",
    "# Assessment 2B-Task1: Flight Producer\n",
    "\n",
    "\n",
    "Student information\n",
    "- Family Name: Aggarwal\n",
    "- Given Name: Naval\n",
    "- Student ID: 31153054\n",
    "- Student email: nagg0001@student.monash.edu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import sleep\n",
    "from json import dumps\n",
    "from kafka import KafkaProducer\n",
    "import random\n",
    "import datetime as dt\n",
    "from datetime import datetime,timezone\n",
    "from pytz import timezone\n",
    "import csv\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysing and Creating Producer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method is used to get flight records\n",
    "def getFlightRecords():        \n",
    "    # Path of files\n",
    "    path = r'datasets/flight*'\n",
    "    # Using glob to extract files\n",
    "    all_flight_files = glob.glob(path + \".csv\")\n",
    "\n",
    "    flight_data = []\n",
    "    \n",
    "    for each_file in all_flight_files:\n",
    "        with open(each_file, 'rt') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            \n",
    "            for row in reader:\n",
    "                flight_data.append({'YEAR':int(row['YEAR']),'MONTH':int(row['MONTH']),'DAY':int(row['DAY']),'DAY_OF_WEEK':int(row['DAY_OF_WEEK']),'AIRLINE':row['AIRLINE'],'FLIGHT_NUMBER':int(row['FLIGHT_NUMBER']),'TAIL_NUMBER':row['TAIL_NUMBER'],'ORIGIN_AIRPORT':row['ORIGIN_AIRPORT'],'DESTINATION_AIRPORT':row['DESTINATION_AIRPORT'],'SCHEDULED_DEPARTURE':int(row['SCHEDULED_DEPARTURE']),'DEPARTURE_TIME':row['DEPARTURE_TIME'],'DEPARTURE_DELAY':row['DEPARTURE_DELAY'],'TAXI_OUT':row['TAXI_OUT'],'WHEELS_OFF':row['WHEELS_OFF'],'SCHEDULED_TIME':row['SCHEDULED_TIME'],'ELAPSED_TIME':row['ELAPSED_TIME'],'AIR_TIME':row['AIR_TIME'],'DISTANCE':row['DISTANCE'],'WHEELS_ON':row['WHEELS_ON'],'TAXI_IN':row['TAXI_IN'],'SCHEDULED_ARRIVAL':int(row['SCHEDULED_ARRIVAL']),'ARRIVAL_TIME':row['ARRIVAL_TIME'],'ARRIVAL_DELAY':row['ARRIVAL_DELAY'],'DIVERTED':int(row['DIVERTED']),'CANCELLED':int(row['CANCELLED'])})\n",
    "            \n",
    "    print(\"The total number of records are\",len(flight_data))\n",
    "    \n",
    "    return flight_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to publish message\n",
    "def publish_message(producer_instance, topic_name, data):\n",
    "    try:\n",
    "        producer_instance.send(topic_name, data)\n",
    "\n",
    "    except Exception as ex:\n",
    "        print('Exception in publishing message.')\n",
    "        print(str(ex))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to connect kafka with producer\n",
    "def connect_kafka_producer():\n",
    "    _producer = None\n",
    "    try:\n",
    "        _producer = KafkaProducer(bootstrap_servers=['localhost:9092'],\n",
    "                                  value_serializer=lambda x: dumps(x).encode('ascii'),\n",
    "                                  api_version=(0, 10))\n",
    "    except Exception as ex:\n",
    "        print('Exception while connecting Kafka.')\n",
    "        print(str(ex))\n",
    "    finally:\n",
    "        return _producer\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "day_list = ['Monday', 'Tuesday','Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "\n",
    "# This method is used to fetch keys based on DAY_OF_WEEK\n",
    "def fetchKeyFlights(data):\n",
    "    keyFlights = dict()\n",
    "\n",
    "    for i in range(1, 8):\n",
    "        keyFlights[i] =  list(filter(lambda y: y['DAY_OF_WEEK'] == i, data))\n",
    "        print(\"Records on \", day_list[i-1], \" are \",  len(keyFlights[i]))\n",
    "        \n",
    "    return keyFlights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2 µs, sys: 1 µs, total: 3 µs\n",
      "Wall time: 5.01 µs\n",
      "The total number of records are 582184\n",
      "Records on  Monday  are  86317\n",
      "Records on  Tuesday  are  84449\n",
      "Records on  Wednesday  are  85607\n",
      "Records on  Thursday  are  87683\n",
      "Records on  Friday  are  86253\n",
      "Records on  Saturday  are  70453\n",
      "Records on  Sunday  are  81422\n",
      "Publishing records..\n"
     ]
    }
   ],
   "source": [
    "%time\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    \n",
    "    #topic name\n",
    "    topic = 'flightTopic'   \n",
    "    \n",
    "    # rows of the csv file\n",
    "    flightRecords = getFlightRecords()\n",
    "    \n",
    "    # connecting\n",
    "    flightProducer = connect_kafka_producer()\n",
    "    \n",
    "    # Creating dictionary based flights occuring on Days \n",
    "    KeyFlights = fetchKeyFlights(flightRecords)\n",
    "    \n",
    "    # Creating a dictionary to store the indexes for data to be send on days basis \n",
    "    start_index_dict = dict()\n",
    "    \n",
    "    \n",
    "    print('Publishing records..')\n",
    "    \n",
    "    # Iterating till we reach the end of dataset\n",
    "    while True:\n",
    "        \n",
    "        # List of rows send in the current batch\n",
    "        publish = []\n",
    "\n",
    "        # List of rows to be sent as late rows\n",
    "        publish_late_data = []\n",
    "\n",
    "        # iteration for each batch X1 and Y1\n",
    "        for key, value in KeyFlights.items():\n",
    "\n",
    "            # Checking the index from where the start of dataset needs to send\n",
    "            if(start_index_dict.get(key) == None):\n",
    "                start_index = 0\n",
    "            else:\n",
    "                start_index = start_index_dict[key]\n",
    "\n",
    "\n",
    "            # creating number of instances of sub batches in current pass\n",
    "            A_random = random.randint(70, 100)\n",
    "\n",
    "            # creating number of instances of sub batches in late pass\n",
    "            B_random = random.randint(5, 10)\n",
    "\n",
    "\n",
    "            # Creating the current timestamp\n",
    "            ts = {'ts': int(dt.datetime.now(timezone('UTC')).timestamp())}\n",
    "\n",
    "            to_send = value[start_index:start_index + A_random]\n",
    "\n",
    "            # increase the start index by the number of rows taken\n",
    "            start_index = start_index + A_random\n",
    "            # append the timestamp into the object to be sent\n",
    "            data = [dict(item, **ts) for item in to_send]\n",
    "\n",
    "            # late data\n",
    "            to_send = value[start_index:start_index + B_random]\n",
    "            # increase the start index by the number of rows taken\n",
    "            start_index = start_index + B_random\n",
    "            # append the timestamp into the object to be sent\n",
    "            late_data = [dict(item, **ts) for item in to_send]\n",
    "\n",
    "\n",
    "            publish.extend(data)\n",
    "\n",
    "            # reseting to start from the beginning\n",
    "            if start_index >= len(value):\n",
    "                start_index = 0\n",
    "            \n",
    "            # Sleeping the thread, so that there is a difference in timestamp for A1,A2,....A7 \n",
    "            sleep(1)\n",
    "            \n",
    "            # Adding the late rows  \n",
    "            publish_late_data.extend(late_data)\n",
    "\n",
    "            # Storing the index for particular day\n",
    "            start_index_dict[key] = start_index\n",
    "\n",
    "\n",
    "\n",
    "        # kafka.publish\n",
    "        publish_message(flightProducer, topic, publish)\n",
    "\n",
    "        # Extending the publish list with the late rows to be send in the next batch\n",
    "        publish.extend(publish_late_data)\n",
    "\n",
    "        # Sleeping the thread for 10 seconds between different batches \n",
    "        sleep(10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
